[
  {
    "objectID": "1752908400",
    "permalink": "/post/%E6%B0%B4%E5%88%A9%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD/",
    "title": "水利专业术语","content": " 反推入库：反推入库流量就是用时段内库容差推出一 场洪水的实际入库过程，本意是还原次降水形成的洪水由起 涨峰顶落平的原始过程。 库容：指水库在某一水位下可储存的水体总体积，通常分为多个层级：\n死库容：最低水位以下无法排出的水量（用于泥沙淤积、生态维持等）。 兴利库容（有效库容）：正常蓄水位与死水位之间的容积，用于供水、发电等效益。 防洪库容：设计洪水位与正常蓄水位之间的容积，用于调蓄洪水。 总库容：校核洪水位以下的全部容积，是水库规模的重要指标（如大、中、小型水库的划分依据）。 径流深：径流深是指在某一时段内通过河流上指定断面的径流总量（W以m^3计）除以该断面以上的流域面积（F，以km²计）所得的值。计算公式为： $$ R = \\frac{W , (\\text{m³})}{1000 \\times F , (\\text{km}^2)} $$ ​\tW为径流总量（m³），F为流域面积（km²）。 该公式通过单位换算将三维径流量转化为二维水深，便于不同流域间的水资源对比。 径流总量：径流总量是描述水文循环中水量平衡的核心物理量，指在指定时段Δt内通过河流某一断面的液态或固态水体积总和。其基本计算方法为时段平均流量与时间的乘积，常用立方米（m³）、亿立方米作为度量单位 。根据径流路径可分为地面径流总量与地下径流总量两种类型，二者共同构成流域水资源评估的基础参数 。该指标在水资源规划、防洪工程设计及生态保护等领域具有重要应用价值 。 流域面积：又称汇水面积或集水面积，流域分水线所包围的面积。流域面积大都先从地形图上定出分水线用求积仪或其它方法量算求得，计算单位为km²。 汛期：汛期是指由于季节性降水或冰雪融化引起的江河水位上涨的时期。一般来说，南方地区的汛期通常为4月至10月，而北方地区的汛期一般为6月至9月。在南方，汛期的主汛期通常从6月1日开始。汛期并不等同于水灾，但水灾通常发生在汛期内\n梅汛期：梅汛期是指由梅雨期降水而引起的江河水位上涨时期。每年时间不固定，一般发生在6月上旬至7月上旬。这是我国长江流域、淮河流域主要的汛期，是一年中流量最大的时期，容易引起灾害。 台汛期：**台汛期是指每年台风影响的时段，通常从5月持续到10月，其中7月至9月最为频繁。**由于台风的影响而导致的汛期，主要表现为强降雨和洪水的发生。台风通常在热带地区形成，带来强风和暴雨，对沿海及内陆地区造 …","date": "2025-07-19 00:00:00",
    "updated": "2025-07-19 00:00:00"
  }, 
  {
    "objectID": "1751007600",
    "permalink": "/post/1.-mysql-%E9%85%8D%E7%BD%AE/",
    "title": "Maxwell 和 MySQL 配置示例","content": " 1. MySQL 配置 编辑 MySQL 配置文件（如 /etc/my.cnf 或 /usr/local/mysql/my.cnf），确保以下配置：\n[mysqld] # 基础设置 datadir=/usr/local/mysql/data port=3306 max_connections=400 sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES lower_case_table_names=1 innodb_file_per_table=1 # binlog 配置 server-id=1 # 每个实例唯一 log-bin=mysql-bin # 启用 binlog 并设置基本名称 binlog_format=row # 设置 binlog 格式为 ROW expire_logs_days=7 # binlog 文件保留 7 天后自动清理 binlog_do_db=shtd_db1 # 仅记录指定数据库的 binlog # 如果有多个数据库，可以重复添加 binlog-do-db 行 # binlog-do-db=shtd_db2 2. 创建 Maxwell 用户 在 MySQL 中执行以下命令，为 Maxwell 创建一个用户并授予必要权限：\nCREATE USER \u0026#39;maxwell\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;; GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO \u0026#39;maxwell\u0026#39;@\u0026#39;%\u0026#39;; FLUSH PRIVILEGES; 3. Maxwell 配置 Maxwell 的配置文件（如 config.properties）通常位于 Maxwell 安装目录中。如果不存在，需要创建。以下是优化后的配置示例：\n# MySQL 配置 host=192.168.1.100 # MySQL 主机地址 user=maxwell # Maxwell 用户 password=password # Maxwell 用户密码 port=3306 # MySQL 端口 # Kafka 配置 producer=kafka # 指定消息投递到 Kafka kafka.bootstrap.servers=192.168.1.200:9092,192.168.1.201:9092 # Kafka 集群地址 kafka_topic=maxwell_shtd_db1 # Kafka Topic 名称 4. 启动与验证 启动 Maxwell 运行以下命令启动 Maxwell：\nbin/maxwell --config config.properties 验证日志 观察 Maxwell 的启动日志，确认是否成功连接到 MySQL 和 Kafka，并输出类似以下信息：\nINFO Connected to MySQL at 192.168.1.100:3306 INFO Kafka producer connected to: 192.168.1.200:9092 INFO Starting to stream changes from binlog mysql-bin.000001:154 验证 Kafka 消息 使用 Kafka 自带的消费者工具查看指定 Topic 是否接收到 Maxwell 发送的消息：\nkafka-console-consumer --bootstrap-server 192.168.1.200:9092 --topic maxwell_shtd_db1 --from-beginning 你应该能看到类似以下格式的 JSON 消息：\n{ \u0026#34;database\u0026#34;: \u0026#34;shtd_db1\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;ts\u0026#34;: 1672394546, \u0026#34;xid\u0026#34;: 12345, \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;alice@example.com\u0026#34; } } 总结 MySQL 配置：启用 binlog，设置 row 格式，明确 binlog 记录的数据库。 Maxwell 配置：确保 Kafka 地址正确，指定 Topic 名称，并优化消息过滤规则。 验证：检查 Maxwell 和 Kafka 是否正常工作，确认消息被正确消费。 如需调整更多细节，例如消息格式或过滤特定数据表，Maxwell 提供灵活的选项支持。\n","date": "2025-06-27 00:00:00",
    "updated": "2025-06-27 00:00:00"
  }, 
  {
    "objectID": "1751007600",
    "permalink": "/post/%E5%AE%9E%E6%97%B6%E9%87%87%E9%9B%86%E6%96%B9%E6%B3%95/",
    "title": "实时采集的方法","content": " 1. 使用 Flume 采集某端口的实时数据流并存入 Kafka 指定的Topic 中 flume的配置 a1.sources=r1 a1.sinks=k1 a1.channels=c1 a1.sources.r1.type=netcat a1.sources.r1.bind=master a1.sources.r1.port=9999 a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.brokerList=master:9092,slave1:9092,slave2:9092 a1.sinks.k1.topic=lol a1.sinks.k1.serializer.class=kafka.serializer.StringEncoding a1.sinks.kafka.producer.max.request.size=10240000 a1.channels.c1.type=memory a1.channels.c1.capacity=100000 a1.channels.c1.transactionCapacity=1000 a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 #如何做 1.启动自己配置的flume文件 2.启动kafka 3.配置kafka主题 4.消费kafka 2. 使用 Maxwell 采集 MySQL 的 binlog 日志并存入 Kafka 指定的Topic 中 Maxwell的配置 在Maxwell中配置到config.properties文件（可能需要自己创建）\n# MySQL 配置 host=\u0026lt;MySQL 主机地址\u0026gt; user=\u0026lt;MySQL 用户名\u0026gt; password=\u0026lt;MySQL 密码\u0026gt; port=3306 # Kafka 配置 producer=kafka kafka.bootstrap.servers=\u0026lt;Kafka 集群地址\u0026gt; kafka_topic=\u0026lt;Kafka Topic 名称\u0026gt; 编辑Mysql配置 文件地址 vi /etc/my.cnf 或/usr/local/mysql/my.cnf\n#确保 MySQL 已启用 binlog [mysqld] # 启用 binlog log-bin=mysql-bin # 设置 binlog 格式为 ROW（推荐） binlog_format=row # 设置 server-id（每个 MySQL 实例必须唯一） server-id=1 log-bin 表示启用 binlog。 binlog_format=row 表示使用行级别的 binlog（Maxwell 必须使用 row 格式）。 server-id 必须设置且唯一，否则无法启用 binlog。 #为 Maxwell 创建一个具有读取 binlog 权限的用户 CREATE USER \u0026#39;maxwell\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;; GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO \u0026#39;maxwell\u0026#39;@\u0026#39;%\u0026#39;; FLUSH PRIVILEGES; 验证 验证 Maxwell 用户权限是否正常： 使用 maxwell 用户连接 MySQL： mysql -u maxwell -p -h \u0026lt;MySQL 主机\u0026gt; 验证是否可以读取 binlog： SHOW MASTER STATUS; 如果能正常返回 binlog 文件名和位置，则说明配置成功。 #如何做 先配置好上面两个 注意配置好mysql需要重新启动 //可以检验一下是mysql是否配置好 1.启动kafka 2.创建kafka主题 3.启动Maxwell 4.启动kafka的消费者 ","date": "2025-06-27 00:00:00",
    "updated": "2025-06-27 00:00:00"
  }, 
  {
    "objectID": "1715796000",
    "permalink": "/post/flink_kafka_job_guide.md/",
    "title": "Flume + Flink Kafka 实时处理任务配置与开发","content": " 一、Flume 采集器配置 a1.sources=r1 a1.channels=c1 a1.sinks=k1 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=26001 a1.channels.c1.type=memory a1.channels.c1.capacity=1000 a1.channels.c1.transactionCapacity=100 a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.brokerList=master:9092,slave1:9092,slave2:9092 a1.sinks.k1.topic=tb_hhh a1.sinks.k1.serializer.class=kafka.serializer.StringEncoder a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 二、Flink 任务开发步骤 1. 构建 Flink 环境 val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(1) ... inputStream.print() env.execute(\u0026#34;JobName\u0026#34;) 2. 确认数据来源 // 测试文件 val inputStream = env.readTextFile(\u0026#34;路径\u0026#34;) // 网络端口 val inputStream = env.socketTextStream(\u0026#34;localhost\u0026#34;, 端口号) // Kafka val props = new Properties() props.setProperty(\u0026#34;bootstrap.servers\u0026#34;, \u0026#34;192.168.136.3:9092,...\u0026#34;) props.setProperty(\u0026#34;group.id\u0026#34;, \u0026#34;aaa\u0026#34;) val inputStream = env.addSource(new FlinkKafkaConsumer[String](\u0026#34;student_hhh\u0026#34;, new SimpleStringSchema(), props)) 3. 数据清洗 val dataStream = inputStream.filter(x =\u0026gt; x.startsWith(\u0026#34;A\u0026#34;)) 4. 数据分组与聚合 val resultStream = dataStream.map(x =\u0026gt; { val arr1 = x.split(\u0026#34;分隔符\u0026#34;) val arr2 = arr1(1).split(\u0026#34;分隔符\u0026#34;) (arr2(0), 1) }).keyBy(_._1).sum(1) 5. 时间窗口（可选） .keyBy(_._1) .timeWindow(Time.minutes(8)) // 滚动窗口 .timeWindow(Time.minutes(10), Time.minutes(3)) // 滑动窗口 6. 数据输出 (1) Redis 字符串形式 val conf = new FlinkJedisPoolConfig.Builder().setHost(\u0026#34;localhost\u0026#34;).setPort(6379).build() dataStream.addSink(new RedisSink[String](conf, new RedisMapper[String] { override def getCommandDescription = new RedisCommandDescription(RedisCommand.SET) override def getKeyFromData(t: String): String = t._1 override def getValueFromData(t: String): String = t._2.toString })) (2) Redis 哈希形式 new RedisCommandDescription(RedisCommand.HSET, \u0026#34;myKey\u0026#34;) (3) 数据存入 MySQL class MysqlSink extends RichSinkFunction[(String, Int)] { var conn: Connection = _ var upPstmt: PreparedStatement = _ var inPstmt: PreparedStatement = _ override def open(parameters: Configuration): Unit = { conn = DriverManager.getConnection(\u0026#34;jdbc:mysql://...\u0026#34;,\u0026#34;root\u0026#34;,\u0026#34;password\u0026#34;) inPstmt = conn.prepareStatement(\u0026#34;INSERT INTO tb_course_score VALUES (NULL,?,?)\u0026#34;) upPstmt = conn.prepareStatement(\u0026#34;UPDATE tb_course_score SET score=? WHERE course=?\u0026#34;) } override def invoke(value: (String, Int), context: SinkFunction.Context): Unit = { upPstmt.setInt(1, value._2) upPstmt.setString(2, value._1) upPstmt.execute() if (upPstmt.getUpdateCount == 0) { inPstmt.setString(1, value._1) inPstmt.setInt(2, value._2) inPstmt.execute() } } override def close(): Unit = { upPstmt.close() inPstmt.close() conn.close() } } (4) 数据写入 ClickHouse class ClickHouseSink extends RichSinkFunction[(String,String,String,Double,String,String,String,String,String)] { var conn: Connection = _ var pstmt: PreparedStatement = _ override def open(parameters: Configuration): Unit = { conn = DriverManager.getConnection(\u0026#34;jdbc:clickhouse://182.168.136.3\u0026#34;,\u0026#34;default\u0026#34;,\u0026#34;123456\u0026#34;) pstmt = conn.prepareStatement(\u0026#34;INSERT INTO tb_hhh VALUES(?,?,?,?,?,?,?,?,?)\u0026#34;) } override def invoke(value: (String, String, String, Double, String, String, String, String, String), context: SinkFunction.Context): Unit = { pstmt.setString(1, value._1) pstmt.setString(2, value._2) pstmt.setString(3, value._3) pstmt.setDouble(4, value._4) pstmt.setString(5, value._5) pstmt.setString(6, value._6) pstmt.setString(7, value._7) pstmt.setString(8, value._8) pstmt.setString(9, value._9) pstmt.execute() } override def close(): Unit = { conn.close() pstmt.close() } } 7. 提交执行任务 打包步骤： 在 IntelliJ IDEA 中构建 -\u0026gt; Build Artifacts -\u0026gt; Rebuild JAR 包路径：out/artifacts/...jar 传输至服务器 /opt/jars/ 执行任务： # Flink 集群方式（需先监听端口） flink run -c com.xxx.MainClass /opt/jars/app.jar # Yarn 方式（需启动 Hadoop） flink run -m yarn-cluster -c com.xxx.MainClass /opt/jars/app.jar # 如果出错： export HADOOP_CLASSPATH=`hadoop classpath` 至此，完整数据采集、处理与存储流程完成。\n","date": "2024-05-15 11:00:00",
    "updated": "2024-05-15 11:00:00"
  }, 
  {
    "objectID": "1715796000",
    "permalink": "/post/vue_flask_guide/",
    "title": "Vue前端 + 后端 Flask 构建","content": " 一、前端构建步骤 1. 安装 Node.js 访问官网下载安装： https://nodejs.org/zh-cn/download 下载 Windows 64 位 .msi 文件，安装时建议不要安装到 C 盘或带中文的目录。 验证安装是否成功：\nnode -v npm -v 2. 升级相关依赖 npm install express --registry=https://registry.npm.taobao.org npm install -g node npm install -g npm npm install -g cnpm --registry=https://registry.npm.taobao.org 3. 安装 vue-cli 脚手架工具 npm install -g @vue/cli vue -V # 查看版本，必须为 4.5.X 以上 如需升级 vue-cli：\nnpm update -g @vue/cli 卸载命令：\nnpm uninstall -g vue-cli 4. 创建 Vue3 项目 cd your_project_path # 进入项目目录（最好使用英文路径） vue create 项目名 5. 启动项目 cd 项目名 npm run serve 6. 使用 VSCode 打开项目，参考网上 Vue 项目目录结构说明 二、后端服务端构建（Flask + MySQL） 1. 准备环境 安装 MySQL 数据库 安装 Python 3.7 安装 PyCharm 专业版 2. 创建 Flask 项目 打开 PyCharm，新建项目 左侧选择 Flask，指定项目目录 3. 编辑 app.py 文件，粘贴如下服务端代码： 导入依赖：\nfrom flask import Flask, request import pymysql import json app = Flask(__name__) 接口1：获取所有数据\n@app.route(\u0026#39;/getBrowserByDate1\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def getBrowserByDate1(): conn = pymysql.connect(host=\u0026#39;127.0.0.1\u0026#39;, user=\u0026#39;root\u0026#39;, passwd=\u0026#39;123456\u0026#39;, db=\u0026#39;shtd_store\u0026#39;, charset=\u0026#39;utf8\u0026#39;) cur = conn.cursor() cur.execute(\u0026#34;SELECT * FROM tb_browsers\u0026#34;) res = cur.fetchall() cur.close() conn.commit() conn.close() arrayResult = [] for row in res: result = { \u0026#39;id\u0026#39;: row[0], \u0026#39;user\u0026#39;: row[1], \u0026#39;code\u0026#39;: row[2], \u0026#39;browser\u0026#39;: row[3], \u0026#39;banner\u0026#39;: row[4], \u0026#39;score_designer\u0026#39;: row[5], \u0026#39;score_speed\u0026#39;: row[6], \u0026#39;score_content\u0026#39;: row[7], \u0026#39;visit_date\u0026#39;: row[8] } arrayResult.append(result) result = { \u0026#39;code\u0026#39;: 200, \u0026#39;msg\u0026#39;: \u0026#39;查询成功\u0026#39;, \u0026#39;results\u0026#39;: arrayResult } return json.dumps(result).encode(\u0026#39;utf-8\u0026#39;).decode(\u0026#39;unicode_escape\u0026#39;) 接口2：按时间段查询数据\n@app.route(\u0026#39;/getBrowserByDate2\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def getBrowserByDate2(): start = request.form.get(\u0026#39;start\u0026#39;) end = request.form.get(\u0026#39;end\u0026#39;) conn = pymysql.connect(host=\u0026#39;127.0.0.1\u0026#39;, user=\u0026#39;root\u0026#39;, passwd=\u0026#39;123456\u0026#39;, db=\u0026#39;shtd_store\u0026#39;, charset=\u0026#39;utf8\u0026#39;) cur = conn.cursor() cur.execute(\u0026#34;SELECT * FROM tb_browsers WHERE visit_date \u0026gt;= \u0026#39;{}\u0026#39; AND visit_date \u0026lt;= \u0026#39;{}\u0026#39;\u0026#34;.format(start, end)) res = cur.fetchall() cur.close() conn.commit() conn.close() arrayResult = [] for row in res: result = { \u0026#39;id\u0026#39;: row[0], \u0026#39;user\u0026#39;: row[1], \u0026#39;code\u0026#39;: row[2], \u0026#39;browser\u0026#39;: row[3], \u0026#39;banner\u0026#39;: row[4], \u0026#39;score_designer\u0026#39;: row[5], \u0026#39;score_speed\u0026#39;: row[6], \u0026#39;score_content\u0026#39;: row[7], \u0026#39;visit_date\u0026#39;: row[8] } arrayResult.append(result) result = { \u0026#39;code\u0026#39;: 200, \u0026#39;msg\u0026#39;: \u0026#39;查询成功\u0026#39;, \u0026#39;results\u0026#39;: arrayResult } return json.dumps(result).encode(\u0026#39;utf-8\u0026#39;).decode(\u0026#39;unicode_escape\u0026#39;) 4. 依赖安装 如果出现 pymysql 或 json 模块导入失败，请使用以下命令安装依赖：\npip install PyMySQL -i https://pypi.tuna.tsinghua.edu.cn/simple 至此，前后端项目搭建完毕。\n","date": "2024-05-15 11:00:00",
    "updated": "2024-05-15 11:00:00"
  }, 
  {
    "objectID": "1678903200",
    "permalink": "/post/flume_kafka_flink_pipeline/",
    "title": "Flume + Kafka + Flink 实时数据处理","content": " 1. Flume 采集数据至 Kafka 步骤一：配置 Flume 监听 20000 端口，将数据打印到控制台测试 配置文件 mytest.conf 内容：\na1.sources=r1 a1.channels=c1 a1.sinks=k1 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=20000 a1.channels.c1.type=memory a1.sinks.k1.type=logger #a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink #a1.sinks.k1.kafka.topic=mytest #a1.sinks.k1.kafka.bootstrap.servers=192.168.44.201:9092,192.168.44.202:9092,192.168.44.203:9092 a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 启动命令：\nflume-ng agent -c conf -f mytest.conf --name a1 -Dflume.root.logger=INFO,console 步骤二：Kafka 设置 启动 Zookeeper：zkServer.sh start 启动 Kafka：kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties 查看主题：kafka-topics.sh --zookeeper master:2181 --list 创建主题： kafka-topics.sh --create --zookeeper master:2181 --replication-factor 1 --partitions 1 --topic mytest 启动 Kafka 消费者查看数据： kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mytest --from-beginning 2. Flink 消费 Kafka 数据并进行计算 需求： 每分钟内各用户有效订单总额写入 MySQL 每分钟内有效订单总数写入 Redis，key 为 total_orders Flink 自定义 Sink 写法： class MySqlSink extends RichSinkFunction[(String, Double)] { var conn: Connection = _ var pstmt: PreparedStatement = _ override def open(parameters: Configuration): Unit = { conn = DriverManager.getConnection(\u0026#34;jdbc:mysql://192.168.6.86:3306/bike_db?characterEncoding=utf-8\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;123456\u0026#34;) pstmt = conn.prepareStatement(\u0026#34;INSERT INTO order_stats(user_id, total_amount) VALUES (?, ?)\u0026#34;) } override def invoke(value: (String, Double), context: SinkFunction.Context[_]): Unit = { pstmt.setString(1, value._1) pstmt.setDouble(2, value._2) pstmt.executeUpdate() } override def close(): Unit = { pstmt.close() conn.close() } } 使用自定义 Sink： resultStream.addSink(new MySqlSink()) 3. Flink 程序提交方式 方式一：Flink 集群运行 flink run -c com.xxx.demo.XXXXDemo /opt/jars/xxxx.jar 可通过 Flink Web UI（默认 8081）查看和取消任务 方式二：Flink on Yarn 运行 flink run -m yarn-cluster -c com.xxx.demo.XXXXDemo /opt/jars/xxxx.jar 如果提示 hadoop-classpath 错误： export HADOOP_CLASSPATH=`hadoop classpath` 停止任务： yarn application -kill application_xxxxx 注意事项 将提供的依赖包放入 flink/lib 下 集群其他机器也要拷贝依赖包 ","date": "2023-03-15 11:00:00",
    "updated": "2023-03-15 11:00:00"
  }, 
  {
    "objectID": "1678903200",
    "permalink": "/post/hello/",
    "title": "Hello World","content": "💘 博麗 霊夢 💘\n","date": "2023-03-15 11:00:00",
    "updated": "2023-03-15 11:00:00"
  }]